# Workflow Analysis and Introspection

This repository contains Python scripts for analyzing code submissions and performing introspection on responses generated by language models. The scripts are designed to be modular and can be integrated into larger workflows.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)

## Introduction

The primary goal of this repository is to provide tools for analyzing code submissions and performing introspection on language model responses. The scripts are built using the LangChain framework and can be easily extended or modified to fit specific use cases.

## Features

- **Code Analysis**: Analyze code submissions for various aspects such as readability, performance, and maintainability.
- **Introspection**: Perform introspection on language model responses to understand the reasoning behind the generated answers.
- **Modular Design**: The scripts are designed to be modular, allowing for easy integration into larger workflows.
- **Extensible**: The scripts can be easily extended or modified to fit specific use cases.

## Installation

To use the scripts in this repository, you need to have Python installed on your system. You can install the required dependencies using pip:

```bash
pip install langchain langgraph pydantic
```

## Usage

### Code Analysis

The code analysis scripts are designed to analyze code submissions for various aspects. The main script is `flow_code_analysis_1.py`. To use the script, you need to set the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.

```bash
export OPENAI_API_BASE_URL="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"
```

You can then run the script using the following command:

```bash
python flow_code_analysis_1.py
```

### Introspection

The introspection scripts are designed to perform introspection on language model responses. The main script is `flow_introspection_1.py`. To use the script, you need to set the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.

```bash
export OPENAI_API_BASE_URL="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"
```

You can then run the script using the following command:

```bash
python flow_introspection_1.py
```

## Examples

The `examples` directory contains various example scripts that demonstrate how to use the LangChain framework to build workflows. These examples cover a range of use cases, including generating jokes, writing stories, and performing arithmetic operations.

To run an example, navigate to the `examples` directory and run the script using Python:

```bash
cd examples
python flow_1.py
```

## Contributing

Contributions are welcome! If you have any suggestions or improvements, please open an issue or submit a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

For more information, please refer to the [LangChain documentation](https://langchain.readthedocs.io/).

## File Descriptions

### `flow_code_analysis_1.py`

This script analyzes code submissions for a specific aspect (e.g., readability). It uses a language model to generate a structured analysis of the code.

- **Tasks**:
  - `analyzer`: Analyzes the code submission for a given aspect.
  - `synthesizer`: Summarizes the analysis results.
- **Workflow**:
  - The `orchestrator_worker` function coordinates the analysis and synthesis tasks.

### `flow_code_analysis_2.py`

This script extends the code analysis workflow to handle multiple aspects in parallel. It uses a state graph to manage the workflow.

- **Tasks**:
  - `analyzer`: Analyzes the code submission for a given aspect.
  - `synthesizer`: Summarizes the analysis results.
- **Workflow**:
  - The workflow is defined using a state graph, with nodes for analyzing, synthesizing, and aggregating results.

### `flow_code_analysis_3.py`

This script further extends the code analysis workflow to handle multiple aspects in parallel and combines the results into a final report.

- **Tasks**:
  - `analyzer`: Analyzes the code submission for a given aspect.
  - `synthesizer`: Summarizes the analysis results.
- **Workflow**:
  - The workflow is defined using a state graph, with nodes for analyzing, synthesizing, and aggregating results in parallel.

### `flow_code_analysis_4.py`

This script introduces a fan-out mechanism to handle multiple aspects in parallel and combines the results into a final report.

- **Tasks**:
  - `analyzer`: Analyzes the code submission for a given aspect.
  - `synthesizer`: Summarizes the analysis results.
- **Workflow**:
  - The workflow uses a fan-out mechanism to process multiple aspects in parallel and aggregates the results.

### `flow_code_analysis_5.py`

This script extends the fan-out mechanism to include custom prompts for each aspect and combines the results into a final report.

- **Tasks**:
  - `analyzer`: Analyzes the code submission for a given aspect using a custom prompt.
  - `synthesizer`: Summarizes the analysis results.
- **Workflow**:
  - The workflow uses a fan-out mechanism with custom prompts for each aspect and aggregates the results.

### `flow_introspection_1.py`

This script performs introspection on language model responses to understand the reasoning behind the generated answers.

- **Tasks**:
  - `initial_response_node`: Generates the initial response from the language model.
  - `introspection_prompt_node`: Creates a prompt for introspection.
  - `introspection_node`: Performs introspection on the initial response.
- **Workflow**:
  - The workflow is defined using a graph, with nodes for generating the initial response, creating the introspection prompt, and performing introspection.

### `flow_introspection_2.py`

This script extends the introspection workflow to include logging and structured output.

- **Tasks**:
  - `initial_response_node`: Generates the initial response from the language model.
  - `introspection_prompt_node`: Creates a prompt for introspection.
  - `introspection_node`: Performs introspection on the initial response.
- **Workflow**:
  - The workflow is defined using a graph, with nodes for generating the initial response, creating the introspection prompt, and performing introspection. It includes logging and structured output.

### `flow_introspection_3.py`

This script further extends the introspection workflow to include error handling and detailed logging.

- **Tasks**:
  - `initial_response_node`: Generates the initial response from the language model.
  - `introspection_prompt_node`: Creates a prompt for introspection.
  - `introspection_node`: Performs introspection on the initial response.
- **Workflow**:
  - The workflow is defined using a graph, with nodes for generating the initial response, creating the introspection prompt, and performing introspection. It includes error handling and detailed logging.

### `examples/eval_lmstudio.py`

This script evaluates the language model by invoking it with a sample prompt.

- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model with a sample prompt.

### `examples/eval_ollama.py`

This script evaluates the Ollama language model by invoking it with a sample prompt.

- **Usage**:
  - Initializes the Ollama language model.
  - Invokes the language model with a sample prompt.

### `examples/flow_1.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes.

- **Tasks**:
  - Generates a joke based on a given topic.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke.

### `examples/flow_1b.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes with structured output.

- **Tasks**:
  - Generates a joke based on a given topic.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke with structured output.

### `examples/flow_2.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating and improving jokes.

- **Tasks**:
  - Generates an initial joke.
  - Improves the joke if it lacks a punchline.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate and improve a joke.

### `examples/flow_3.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes, stories, and poems in parallel.

- **Tasks**:
  - Generates a joke, story, and poem based on a given topic.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke, story, and poem in parallel.

### `examples/flow_4.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes, stories, and poems with routing logic.

- **Tasks**:
  - Routes the input to generate a joke, story, or poem based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke, story, or poem based on routing logic.

### `examples/flow_4b.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes, stories, and poems with routing logic using a different language model.

- **Tasks**:
  - Routes the input to generate a joke, story, or poem based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke, story, or poem based on routing logic.

### `examples/flow_4c.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating jokes, stories, and poems with routing logic using a different approach.

- **Tasks**:
  - Routes the input to generate a joke, story, or poem based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a joke, story, or poem based on routing logic.

### `examples/flow_5.py`

This script demonstrates how to use the LangChain framework to build a workflow for performing arithmetic operations using tools.

- **Tasks**:
  - Performs arithmetic operations based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to perform arithmetic operations using tools.

### `examples/flow_6.py`

This script demonstrates how to use the LangChain framework to build a workflow for performing arithmetic operations using tools with a different approach.

- **Tasks**:
  - Performs arithmetic operations based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to perform arithmetic operations using tools.

### `examples/flow_7.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating a report based on a given topic.

- **Tasks**:
  - Generates a plan for the report.
  - Writes sections of the report based on the plan.
  - Synthesizes the final report from the sections.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate a report.

### `examples/flow_8.py`

This script demonstrates how to use the LangChain framework to build a workflow for optimizing jokes based on feedback.

- **Tasks**:
  - Generates a joke based on a given topic.
  - Evaluates the joke and provides feedback.
  - Optimizes the joke based on the feedback.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate and optimize a joke.

### `examples/flow_9.py`

This script demonstrates how to use the LangChain framework to build a workflow for generating and evaluating answers to questions.

- **Tasks**:
  - Generates an answer to a given question.
  - Evaluates the answer and provides feedback.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to generate and evaluate an answer.

### `examples/flow_10.py`

This script demonstrates how to use the LangChain framework to build a workflow for performing arithmetic operations using tools with a different approach.

- **Tasks**:
  - Performs arithmetic operations based on the user's request.
- **Usage**:
  - Sets the `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` environment variables.
  - Invokes the language model to perform arithmetic operations using tools.
